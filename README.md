# Gemma Fine-Tuning with Keras

Welcome to the **Gemma Fine-Tuning with Keras** repository! In this project, we explore fine-tuning Gemma 2B models using Keras for downstream tasks. Gemma models are lightweight, state-of-the-art open models built from the same research and technology used to create the Gemini models.

## Article Reference
To understand each step in detail, refer to this article: [Fine-Tuning Gemma 2B for Downstream Tasks with Keras](https://medium.com/@mauryaanoop3/fine-tuning-gemma-2b-for-downstream-tasks-with-keras-c3314e329be5).

## Dataset
We use the [Databricks Dolly 15k dataset](https://huggingface.co/datasets/databricks/databricks-dolly-15k), which contains 15,000 high-quality human-generated prompt/response pairs specifically designed for fine-tuning large language models (LLMs) like Gemma.

## Notebooks
- `gemma-2b-finetuning-kaggle-notebook.ipynb`: A Kaggle notebook demonstrating Gemma 2B fine-tuning.
- `gemma_2b_finetuning_colab_notebook.ipynb`: A Colab notebook for fine-tuning Gemma 2B.

Feel free to explore and adapt this repository based on your specific use case. Happy fine-tuning! ðŸš€ðŸ¤–


